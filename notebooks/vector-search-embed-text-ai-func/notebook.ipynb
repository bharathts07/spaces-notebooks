{
  "cells": [
    {
      "id": "497ac86e",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(124, 195, 235, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/clouds.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Vector search with AI Function EMBED_TEXT</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n",
        "    <div>\n",
        "        <p><b>Note</b></p>\n",
        "        <p>You can use your existing Standard or Premium workspace with this Notebook.</p>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "\n",
        "The AI functions are currently in **Private Preview**. Please reach out to support@singlestore.com to confirm if this feature can be enabled in your org.\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "This Jupyter notebook demonstrates an **end-to-end vector search pipeline** using SingleStore:\n",
        "\n",
        "1. **Data Ingestion** \u2014 Downloads an example Amazon food reviews dataset and loads it into a SingleStore table\n",
        "2. **Embedding Generation** \u2014 Uses the SingleStore AI function [`EMBED_TEXT()`](https://docs.singlestore.com/cloud/ai/ai-ml-functions/ai-functions/) to generate vector embeddings via a simple SQL call\n",
        "3. **Vector Indexing** \u2014 Creates an [HNSW vector index](https://docs.singlestore.com/cloud/vectors/vector-indexing/) on the embedding column for fast approximate nearest-neighbor (ANN) search\n",
        "4. **Semantic Search** \u2014 Performs vector similarity search using `DOT_PRODUCT()` to find reviews that are semantically similar to a natural-language query\n",
        "5. **API Deployment** \u2014 Deploys a FastAPI endpoint as a [SingleStore Cloud Function](https://docs.singlestore.com/cloud/container-services/cloud-functions/) for production-ready semantic search\n",
        "\n",
        "## Why SingleStore for Vector Search?\n",
        "\n",
        "- **Simple SQL-based embeddings**: `EMBED_TEXT()` is tightly integrated into the SQL layer \u2014 you call it like any other SQL function. No need to set up external embedding services, manage API keys for third-party providers, or write client-side embedding code. The embedding logic runs on SingleStore's **Aura compute** infrastructure, so model management is handled for you.\n",
        "- **HNSW indexing**: Approximate nearest-neighbor search reduces query time from O(n) full-scan to sub-linear, critical for large datasets\n",
        "- **Unified platform**: One database for transactional, analytical, and vector workloads \u2014 no need to sync data to a separate vector database\n",
        "\n",
        "**Prerequisites**: Ensure AI Functions are installed on your deployment (AI > AI & ML Functions)."
      ],
      "id": "8c866e48"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "108b7f78",
      "metadata": {},
      "source": [
        "## Step 1: Configuration\n",
        "\n",
        "Define key parameters for the pipeline. These control dataset size, embedding model, and database targets.\n",
        "\n",
        "| Parameter | Purpose |\n",
        "|-----------|---------|\n",
        "| `SAMPLE_SIZE` | Number of reviews to process. Start small (1,000) for testing, scale up for production. |\n",
        "| `EMBEDDING_MODEL` | The model used by `EMBED_TEXT()`. `titan-embed-text-v2-0` produces 1024-dimensional vectors optimized for semantic similarity. |\n",
        "| `MAX_EMBEDDING_TOKENS` | Token limit per text. Embedding models have fixed context windows \u2014 truncating to 512 tokens ensures quality and avoids silent truncation by the model. |\n",
        "| `RECREATE_DB` | Set to `True` to drop and rebuild everything from scratch. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\u2713 Configuration: 10,000 rows | Model: titan-embed-text-v2-0 | Max tokens: 512\n"
        }
      ],
      "source": [
        "SAMPLE_SIZE = 10000\n",
        "EMBEDDING_MODEL = 'titan-embed-text-v2-0'\n",
        "VECTOR_DIMENSIONS = 1024\n",
        "DATABASE_NAME = 'vector_search_demo'\n",
        "TABLE_NAME = 'product_reviews'\n",
        "RECREATE_DB = False\n",
        "\n",
        "MAX_EMBEDDING_TOKENS = 512  # Optimal token limit for embedding model quality\n",
        "APPROX_CHARS_PER_TOKEN = 4  # ~4 chars per token for English text\n",
        "\n",
        "TEST_VECTOR_SEARCH = False # change this value to run example vector search\n",
        "\n",
        "print(f\"\u2713 Configuration: {SAMPLE_SIZE:,} rows | Model: {EMBEDDING_MODEL} | Max tokens: {MAX_EMBEDDING_TOKENS}\")"
      ],
      "id": "913c3bc3"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aad05571",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies & Import Libraries\n",
        "\n",
        "We need:\n",
        "- **`kagglehub`** \u2014 to download the Amazon Fine Food Reviews dataset directly from Kaggle\n",
        "- **`singlestoredb`** \u2014 the official SingleStore Python SDK (provides connections, cursor API, and cloud function deployment via `singlestoredb.apps`)\n",
        "- **`fastapi` / `pydantic`** \u2014 to define the search API that will be deployed as a SingleStore Cloud Function\n",
        "- **`tqdm`** \u2014 for progress bars during batch operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\ud83d\udce6 Installing dependencies...\n\u2713 Dependencies installed in 4.3s\n\u2713 Libraries imported in 0.07s\n\u2713 Total setup time: 4.3s\n"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Track total workflow time from the very start\n",
        "start_time = time.time()\n",
        "\n",
        "# Track library installation time\n",
        "install_start = time.time()\n",
        "print(\"\ud83d\udce6 Installing dependencies...\")\n",
        "\n",
        "!pip install -q kagglehub singlestoredb fastapi pydantic tqdm\n",
        "\n",
        "install_time = time.time() - install_start\n",
        "print(f\"\u2713 Dependencies installed in {install_time:.1f}s\")\n",
        "\n",
        "# Import libraries\n",
        "import_start = time.time()\n",
        "\n",
        "import pandas as pd\n",
        "import sqlalchemy as sa\n",
        "import singlestoredb as s2\n",
        "import kagglehub\n",
        "import concurrent.futures\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import_time = time.time() - import_start\n",
        "\n",
        "print(f\"\u2713 Libraries imported in {import_time:.2f}s\")\n",
        "print(f\"\u2713 Total setup time: {(install_time + import_time):.1f}s\")"
      ],
      "id": "8467b939"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "03cfac76",
      "metadata": {},
      "source": [
        "## Step 3: Check Current State (Idempotent Pipeline)\n",
        "\n",
        "Before doing any work, we check what already exists in the database. This makes the notebook **idempotent** \u2014 you can re-run it safely without duplicating data or re-generating embeddings.\n",
        "\n",
        "The state check inspects:\n",
        "- Whether the **database** and **table** exist\n",
        "- How many **rows** are loaded vs. needed\n",
        "- How many rows already have **embeddings**\n",
        "- Whether the **vector index** has been created\n",
        "\n",
        "Only the steps that are actually needed will execute in subsequent cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "======================================================================\nCURRENT STATE\n======================================================================\nDatabase exists:     True\nTable exists:        True\nRows in table:       10,000\nEmbedded rows:       5,250\nVector index:        Yes\nState check time:    816.31 ms\n======================================================================\n\nACTIONS NEEDED:\n  \u2713 Data ingestion \n  \u2192 Embedding generation (need 4,750 embeddings)\n  \u2713 Vector index creation\n======================================================================\n"
        }
      ],
      "source": [
        "# Check what's already done using direct connection\n",
        "state_check_start = time.time()\n",
        "\n",
        "conn = s2.connect()\n",
        "\n",
        "try:\n",
        "    with conn.cursor() as cur:\n",
        "\n",
        "        # Drop DB\n",
        "        if RECREATE_DB:\n",
        "            print(f\"  \u26a0\ufe0f  Dropping existing database...\")\n",
        "            cur.execute(f\"DROP DATABASE IF EXISTS {DATABASE_NAME}\")\n",
        "\n",
        "        # Check database exists\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT COUNT(*) FROM information_schema.schemata\n",
        "            WHERE schema_name = '{DATABASE_NAME}'\n",
        "        \"\"\")\n",
        "        db_exists = cur.fetchone()[0] > 0\n",
        "\n",
        "        # Initialize defaults\n",
        "        table_exists = False\n",
        "        total_rows = 0\n",
        "        embedded_rows = 0\n",
        "        index_exists = False\n",
        "\n",
        "        if db_exists:\n",
        "            try:\n",
        "                # Try to use the database\n",
        "                cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "\n",
        "                # Check table exists\n",
        "                cur.execute(f\"\"\"\n",
        "                    SELECT COUNT(*) FROM information_schema.tables\n",
        "                    WHERE table_schema = '{DATABASE_NAME}' AND table_name = '{TABLE_NAME}'\n",
        "                \"\"\")\n",
        "                table_exists = cur.fetchone()[0] > 0\n",
        "\n",
        "                if table_exists:\n",
        "                    # Check data\n",
        "                    cur.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME}\")\n",
        "                    total_rows = cur.fetchone()[0]\n",
        "\n",
        "                    # Check embeddings\n",
        "                    cur.execute(f\"\"\"\n",
        "                        SELECT COUNT(*) FROM {TABLE_NAME} WHERE review_embedding IS NOT NULL\n",
        "                    \"\"\")\n",
        "                    embedded_rows = cur.fetchone()[0]\n",
        "\n",
        "                    # Check vector index\n",
        "                    cur.execute(f\"\"\"\n",
        "                        SELECT COUNT(*) FROM information_schema.statistics\n",
        "                        WHERE table_schema = '{DATABASE_NAME}'\n",
        "                        AND table_name = '{TABLE_NAME}'\n",
        "                        AND index_name = 'vindex_review_embedding'\n",
        "                    \"\"\")\n",
        "                    index_exists = cur.fetchone()[0] > 0\n",
        "\n",
        "            except Exception as e:\n",
        "                # If we can't USE the database, treat it as non-existent\n",
        "                print(f\"\u26a0\ufe0f  Database exists but cannot be accessed: {e}\")\n",
        "                print(f\"   Will recreate database: {DATABASE_NAME}\")\n",
        "                db_exists = False\n",
        "                table_exists = False\n",
        "                total_rows = 0\n",
        "                embedded_rows = 0\n",
        "                index_exists = False\n",
        "finally:\n",
        "    conn.close()\n",
        "\n",
        "# Determine what needs to be done\n",
        "needs_data = total_rows < SAMPLE_SIZE\n",
        "needs_embedding = embedded_rows < SAMPLE_SIZE\n",
        "needs_index = not index_exists\n",
        "\n",
        "state_check_time = time.time() - state_check_start\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CURRENT STATE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Database exists:     {db_exists}\")\n",
        "print(f\"Table exists:        {table_exists}\")\n",
        "print(f\"Rows in table:       {total_rows:,}\")\n",
        "print(f\"Embedded rows:       {embedded_rows:,}\")\n",
        "print(f\"Vector index:        {'Yes' if index_exists else 'No'}\")\n",
        "print(f\"State check time:    {state_check_time*1000:.2f} ms\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nACTIONS NEEDED:\")\n",
        "print(f\"  {'\u2713' if not needs_data else '\u2192'} Data ingestion {'' if not needs_data else f'(need {SAMPLE_SIZE - total_rows:,} more rows)'}\")\n",
        "print(f\"  {'\u2713' if not needs_embedding else '\u2192'} Embedding generation {'' if not needs_embedding else f'(need {SAMPLE_SIZE - embedded_rows:,} embeddings)'}\")\n",
        "print(f\"  {'\u2713' if not needs_index else '\u2192'} Vector index creation\")\n",
        "print(\"=\" * 70)"
      ],
      "id": "2796ee7c"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aea23284",
      "metadata": {},
      "source": [
        "## Step 4: Create Database & Table\n",
        "\n",
        "Creates the database and table if they don't already exist. Key design choices:\n",
        "\n",
        "- **`VECTOR(1024)`** column type stores fixed-dimension float vectors natively in SingleStore \u2014 no serialization/deserialization overhead\n",
        "- **`review_text_for_embedding`** stores the token-truncated text that was actually embedded (important for reproducibility)\n",
        "- **`SORT KEY (id)`** optimizes sequential scans during batch processing\n",
        "- **Secondary keys** on `product_id` and `score` enable fast filtered queries alongside vector search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n\u2713 Database and table already exist - skipping creation\n"
        }
      ],
      "source": [
        "if not db_exists or not table_exists:\n",
        "    setup_start = time.time()\n",
        "    print(\"\\n\ud83d\udce6 Setting up database and table...\")\n",
        "\n",
        "    conn = s2.connect()\n",
        "    try:\n",
        "        with conn.cursor() as cur:\n",
        "            # Drop and recreate if there were access issues\n",
        "            if db_exists and not table_exists:\n",
        "                print(f\"  \u26a0\ufe0f  Dropping existing inaccessible database...\")\n",
        "                cur.execute(f\"DROP DATABASE IF EXISTS {DATABASE_NAME}\")\n",
        "\n",
        "            # Create database\n",
        "            cur.execute(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
        "            cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "\n",
        "            # Create table\n",
        "            cur.execute(f\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
        "                    id BIGINT PRIMARY KEY,\n",
        "                    product_id VARCHAR(50),\n",
        "                    user_id VARCHAR(50),\n",
        "                    profile_name VARCHAR(255),\n",
        "                    score INT,\n",
        "                    review_time BIGINT,\n",
        "                    summary TEXT,\n",
        "                    review_text TEXT,\n",
        "                    full_review_text TEXT,\n",
        "                    review_text_for_embedding TEXT,\n",
        "                    review_embedding VECTOR({VECTOR_DIMENSIONS}) NULL,\n",
        "                    KEY (product_id),\n",
        "                    KEY (score),\n",
        "                    SORT KEY (id)\n",
        "                )\n",
        "            \"\"\")\n",
        "        conn.commit()\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    setup_time = time.time() - setup_start\n",
        "    print(f\"\u2713 Database and table created in {setup_time:.2f}s\")\n",
        "else:\n",
        "    print(\"\\n\u2713 Database and table already exist - skipping creation\")"
      ],
      "id": "1ffc930b"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "30f5ac07",
      "metadata": {},
      "source": [
        "## Step 5: Download & Ingest Data\n",
        "\n",
        "Downloads the [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews) dataset from Kaggle and inserts it into SingleStore.\n",
        "\n",
        "**Token-aware truncation**: Embedding models have a maximum input length (measured in tokens, not characters). Text beyond the limit is silently truncated by the model, which can degrade embedding quality. We pre-truncate at word boundaries to approximately `MAX_EMBEDDING_TOKENS` tokens (~4 characters per token for English) so we control exactly what gets embedded.\n",
        "\n",
        "Data is inserted in batches of 1,000 rows for efficient network utilization without hitting memory limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n\u2713 Already have 10,000 rows - skipping data ingestion\n"
        }
      ],
      "source": [
        "if needs_data:\n",
        "    data_start = time.time()\n",
        "    rows_needed = SAMPLE_SIZE - total_rows\n",
        "    print(f\"\\n\ud83d\udce5 Downloading and inserting {rows_needed:,} rows...\")\n",
        "\n",
        "    # Download dataset\n",
        "    download_start = time.time()\n",
        "    path = kagglehub.dataset_download(\"snap/amazon-fine-food-reviews\")\n",
        "    df = pd.read_csv(f\"{path}/Reviews.csv\")\n",
        "    download_time = time.time() - download_start\n",
        "    print(f\"  \u2713 Downloaded in {download_time:.1f}s\")\n",
        "\n",
        "    # Prepare data\n",
        "    prep_start = time.time()\n",
        "    sample_df = df.iloc[total_rows:SAMPLE_SIZE].copy()\n",
        "    sample_df['full_review_text'] = sample_df['Summary'].fillna('') + '. ' + sample_df['Text'].fillna('')\n",
        "\n",
        "    # --- Token-aware truncation for embedding quality ---\n",
        "    max_chars = MAX_EMBEDDING_TOKENS * APPROX_CHARS_PER_TOKEN  # e.g. 512 * 4 = 2048 chars\n",
        "\n",
        "    def truncate_to_token_limit(text, max_chars):\n",
        "        \"\"\"Truncate text to approximate token limit, breaking at word boundaries.\"\"\"\n",
        "        text = str(text).strip()\n",
        "        if len(text) <= max_chars:\n",
        "            return text\n",
        "        # Truncate at last word boundary before the limit\n",
        "        truncated = text[:max_chars]\n",
        "        last_space = truncated.rfind(' ')\n",
        "        if last_space > max_chars * 0.8:  # Only break at word boundary if reasonable\n",
        "            truncated = truncated[:last_space]\n",
        "        return truncated.rstrip()\n",
        "\n",
        "    sample_df['review_text_for_embedding'] = sample_df['full_review_text'].apply(\n",
        "        lambda x: truncate_to_token_limit(x, max_chars)\n",
        "    )\n",
        "\n",
        "    # Report truncation stats\n",
        "    original_lengths = sample_df['full_review_text'].str.len()\n",
        "    truncated_lengths = sample_df['review_text_for_embedding'].str.len()\n",
        "    num_truncated = (original_lengths > max_chars).sum()\n",
        "    print(f\"  \u2702\ufe0f  Token truncation (max {MAX_EMBEDDING_TOKENS} tokens \u2248 {max_chars} chars):\")\n",
        "    print(f\"    Truncated: {num_truncated:,}/{len(sample_df):,} reviews ({num_truncated/len(sample_df)*100:.1f}%)\")\n",
        "    print(f\"    Original avg:  {original_lengths.mean():.0f} chars | Truncated avg: {truncated_lengths.mean():.0f} chars\")\n",
        "\n",
        "    sample_df = sample_df.fillna({'ProductId': '', 'UserId': '', 'ProfileName': '', 'Score': 0})\n",
        "\n",
        "    # Rename columns\n",
        "    sample_df = sample_df.rename(columns={\n",
        "        'Id': 'id', 'ProductId': 'product_id', 'UserId': 'user_id',\n",
        "        'ProfileName': 'profile_name', 'Score': 'score', 'Time': 'review_time',\n",
        "        'Summary': 'summary', 'Text': 'review_text'\n",
        "    })\n",
        "    prep_time = time.time() - prep_start\n",
        "    print(f\"  \u2713 Prepared data in {prep_time:.1f}s\")\n",
        "\n",
        "    # Insert with progress bar\n",
        "    insert_start = time.time()\n",
        "    conn = s2.connect()\n",
        "\n",
        "    try:\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "\n",
        "            batch_size = 1000\n",
        "            num_batches = (len(sample_df) + batch_size - 1) // batch_size\n",
        "\n",
        "            # Progress bar for insertion\n",
        "            with tqdm(total=len(sample_df), desc=\"  Inserting rows\", unit=\"rows\") as pbar:\n",
        "                for i in range(0, len(sample_df), batch_size):\n",
        "                    batch = sample_df.iloc[i:i+batch_size]\n",
        "\n",
        "                    # Build multi-row insert\n",
        "                    values = []\n",
        "                    for _, row in batch.iterrows():\n",
        "                        # Escape single quotes\n",
        "                        product_id = str(row['product_id']).replace(\"'\", \"''\")\n",
        "                        user_id = str(row['user_id']).replace(\"'\", \"''\")\n",
        "                        profile_name = str(row['profile_name']).replace(\"'\", \"''\")\n",
        "                        summary = str(row['summary']).replace(\"'\", \"''\")\n",
        "                        review_text = str(row['review_text']).replace(\"'\", \"''\")\n",
        "                        full_text = str(row['full_review_text']).replace(\"'\", \"''\")\n",
        "                        text_for_embedding = str(row['review_text_for_embedding']).replace(\"'\", \"''\")\n",
        "\n",
        "                        values.append(\n",
        "                            f\"({row['id']}, '{product_id}', '{user_id}', \"\n",
        "                            f\"'{profile_name}', {row['score']}, {row['review_time']}, \"\n",
        "                            f\"'{summary}', '{review_text}', '{full_text}', '{text_for_embedding}')\"\n",
        "                        )\n",
        "\n",
        "                    insert_query = f\"\"\"\n",
        "                        INSERT INTO {TABLE_NAME}\n",
        "                        (id, product_id, user_id, profile_name, score, review_time,\n",
        "                         summary, review_text, full_review_text, review_text_for_embedding)\n",
        "                        VALUES {','.join(values)}\n",
        "                    \"\"\"\n",
        "                    cur.execute(insert_query)\n",
        "                    pbar.update(len(batch))\n",
        "\n",
        "        conn.commit()\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    insert_time = time.time() - insert_start\n",
        "    data_time = time.time() - data_start\n",
        "\n",
        "    print(f\"\\n  \u2713 Inserted {rows_needed:,} rows\")\n",
        "    print(f\"    Download:  {download_time:.1f}s\")\n",
        "    print(f\"    Prepare:   {prep_time:.1f}s\")\n",
        "    print(f\"    Insert:    {insert_time:.1f}s ({rows_needed/insert_time:.0f} rows/sec)\")\n",
        "    print(f\"    Total:     {data_time:.1f}s\")\n",
        "else:\n",
        "    print(f\"\\n\u2713 Already have {total_rows:,} rows - skipping data ingestion\")"
      ],
      "id": "7e24bda4"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44767706",
      "metadata": {},
      "source": [
        "## Step 6: Generate Embeddings with `EMBED_TEXT()`\n",
        "\n",
        "This is the core AI step. Instead of writing Python code to call an external embedding API, we use SingleStore's **`EMBED_TEXT()`** function to generate embeddings with a simple SQL call.\n",
        "\n",
        "### How `EMBED_TEXT()` works\n",
        "```sql\n",
        "EMBED_TEXT(text_column, model => 'titan-embed-text-v2-0', tokens_threshold => 2000000)\n",
        "```\n",
        "- Integrated directly into SingleStore's SQL layer : you call it like any other function in a `SELECT` or `INSERT` statement\n",
        "- The embedding computation runs on SingleStore's **Aura compute** infrastructure, which manages the model hosting and scaling for you\n",
        "- The `tokens_threshold` parameter controls batching: the function accumulates rows until it reaches this token count, then sends them to the model in one batch (higher = more throughput, but more memory)\n",
        "- Returns a JSON array of floats, which we unpack with `JSON_ARRAY_UNPACK_F64()` and cast to `VECTOR(1024)`\n",
        "\n",
        "### Why this is simpler than external embedding APIs\n",
        "With traditional approaches, you'd need to: extract text from the database \u2192 call an external API (OpenAI, Cohere, etc.) \u2192 manage rate limits and retries \u2192 write embeddings back. With `EMBED_TEXT()`, it's a single SQL statement.\n",
        "\n",
        "### Why parallel batch processing?\n",
        "We use 50 worker threads, each processing 10 rows at a time. This saturates the embedding throughput capacity. The `INSERT ... ON DUPLICATE KEY UPDATE` pattern lets us safely upsert, if a row already has an embedding, it gets overwritten (enabling retries on failure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n\ud83d\ude80 Generating 4,750 embeddings in parallel...\n   Using token-truncated text (max 512 tokens) for optimal embedding quality\n  \ud83d\udccf Content size statistics (truncated text):\n    Total content:    2,059,370.0 chars (~514,842 estimated tokens)\n    Avg per row:      434 chars (~108 tokens)\n    Range:            76.0 - 2,050.0 chars\n  Configuration: 10 rows/batch \u00d7 50 workers = 475 batches\n"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0de59ced2a1f44f9a8afbccc631614ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "  Processing:   0%|          | 0/475 [00:00<?, ?batch/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n  \u2713 Generated 4,750 embeddings in 98.8s\n    Throughput:  48 embeddings/sec\n    Avg latency: 20.80 ms/embedding\n    Content processed: 2,059,370.0 chars (~514,842 tokens)\n    Token throughput:  5,212 tokens/sec (20,848 chars/sec)\n"
        }
      ],
      "source": [
        "if needs_embedding:\n",
        "    embedding_total_start = time.time()\n",
        "    rows_to_embed = SAMPLE_SIZE - embedded_rows\n",
        "    print(f\"\\n\ud83d\ude80 Generating {rows_to_embed:,} embeddings in parallel...\")\n",
        "    print(f\"   Using token-truncated text (max {MAX_EMBEDDING_TOKENS} tokens) for optimal embedding quality\")\n",
        "\n",
        "    # Get text content size statistics for token throughput calculation\n",
        "    content_stats_conn = s2.connect()\n",
        "    try:\n",
        "        with content_stats_conn.cursor() as cur:\n",
        "            cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "            cur.execute(f\"\"\"\n",
        "                SELECT\n",
        "                    SUM(LENGTH(review_text_for_embedding)) as total_chars,\n",
        "                    AVG(LENGTH(review_text_for_embedding)) as avg_chars,\n",
        "                    MIN(LENGTH(review_text_for_embedding)) as min_chars,\n",
        "                    MAX(LENGTH(review_text_for_embedding)) as max_chars\n",
        "                FROM {TABLE_NAME}\n",
        "                WHERE review_embedding IS NULL\n",
        "            \"\"\")\n",
        "            stats = cur.fetchone()\n",
        "            total_content_chars = float(stats[0] or 0)\n",
        "            avg_content_chars = float(stats[1] or 0)\n",
        "            min_content_chars = float(stats[2] or 0)\n",
        "            max_content_chars = float(stats[3] or 0)\n",
        "            # Rough estimate: 1 token \u2248 4 characters for English text\n",
        "            estimated_total_tokens = total_content_chars / APPROX_CHARS_PER_TOKEN\n",
        "    finally:\n",
        "        content_stats_conn.close()\n",
        "\n",
        "    print(f\"  \ud83d\udccf Content size statistics (truncated text):\")\n",
        "    print(f\"    Total content:    {total_content_chars:,} chars (~{estimated_total_tokens:,.0f} estimated tokens)\")\n",
        "    print(f\"    Avg per row:      {avg_content_chars:,.0f} chars (~{avg_content_chars/APPROX_CHARS_PER_TOKEN:,.0f} tokens)\")\n",
        "    print(f\"    Range:            {min_content_chars:,} - {max_content_chars:,} chars\")\n",
        "\n",
        "    INSERT_SQL = f\"\"\"\n",
        "    INSERT INTO {TABLE_NAME} (id, review_embedding)\n",
        "    SELECT\n",
        "        id,\n",
        "        JSON_ARRAY_UNPACK_F64(\n",
        "            cluster.EMBED_TEXT(review_text_for_embedding, model => '{EMBEDDING_MODEL}', tokens_threshold => 2000000)\n",
        "        ) :> VECTOR({VECTOR_DIMENSIONS})\n",
        "    FROM (\n",
        "        WITH numbered AS (\n",
        "            SELECT ROW_NUMBER() OVER (ORDER BY id) AS rn, id, review_text_for_embedding\n",
        "            FROM {TABLE_NAME}\n",
        "            WHERE review_embedding IS NULL\n",
        "        )\n",
        "        SELECT * FROM numbered WHERE rn BETWEEN %s AND %s\n",
        "    )\n",
        "    ON DUPLICATE KEY UPDATE review_embedding = VALUES(review_embedding);\n",
        "    \"\"\"\n",
        "\n",
        "    from queue import Queue\n",
        "    import threading\n",
        "\n",
        "    # OPTIMIZED: Small batches + Many workers = True parallelism\n",
        "    batch_size = 10       # Small batches for maximum parallelism\n",
        "    max_workers = 50      # High worker count to saturate cluster\n",
        "\n",
        "    # Create connection pool\n",
        "    connection_pool = Queue(maxsize=max_workers)\n",
        "\n",
        "    pool_start = time.time()\n",
        "    for _ in range(max_workers):\n",
        "        conn = s2.connect()\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "        connection_pool.put(conn)\n",
        "    pool_time = time.time() - pool_start\n",
        "\n",
        "    def run_insert_range(rn_start: int, rn_end: int):\n",
        "        conn = connection_pool.get()\n",
        "        try:\n",
        "            with conn.cursor() as cursor:\n",
        "                cursor.execute(INSERT_SQL, (rn_start, rn_end))\n",
        "            conn.commit()\n",
        "            return (rn_start, rn_end, None)\n",
        "        except Exception as e:\n",
        "            return (rn_start, rn_end, str(e)[:100])\n",
        "        finally:\n",
        "            connection_pool.put(conn)\n",
        "\n",
        "    # Create batches\n",
        "    ranges = [(i, min(i + batch_size - 1, rows_to_embed))\n",
        "              for i in range(1, rows_to_embed + 1, batch_size)]\n",
        "\n",
        "    embed_start = time.time()\n",
        "    errors = []\n",
        "\n",
        "    print(f\"  Configuration: {batch_size} rows/batch \u00d7 {max_workers} workers = {len(ranges)} batches\")\n",
        "\n",
        "    # Progress bar\n",
        "    with tqdm(total=len(ranges), desc=\"  Processing\", unit=\"batch\") as pbar:\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = {executor.submit(run_insert_range, start, end): (start, end)\n",
        "                      for (start, end) in ranges}\n",
        "\n",
        "            for future in concurrent.futures.as_completed(futures):\n",
        "                start, end, err = future.result()\n",
        "                if err:\n",
        "                    errors.append((start, end, err))\n",
        "\n",
        "                elapsed = time.time() - embed_start\n",
        "                embeddings_done = (pbar.n + 1) * batch_size\n",
        "                rate = embeddings_done / elapsed if elapsed > 0 else 0\n",
        "\n",
        "                pbar.set_postfix({'rate': f'{rate:.0f} emb/s'})\n",
        "                pbar.update(1)\n",
        "\n",
        "    embed_time = time.time() - embed_start\n",
        "\n",
        "    # Cleanup\n",
        "    while not connection_pool.empty():\n",
        "        conn = connection_pool.get()\n",
        "        conn.close()\n",
        "\n",
        "    # Summary\n",
        "    print()\n",
        "    if errors:\n",
        "        print(f\"  \u26a0\ufe0f  {len(errors)}/{len(ranges)} batches failed ({len(errors)/len(ranges)*100:.1f}%)\")\n",
        "\n",
        "    actual_throughput = rows_to_embed / embed_time\n",
        "    chars_per_sec = total_content_chars / embed_time if embed_time > 0 else 0\n",
        "    tokens_per_sec = estimated_total_tokens / embed_time if embed_time > 0 else 0\n",
        "    print(f\"  \u2713 Generated {rows_to_embed:,} embeddings in {embed_time:.1f}s\")\n",
        "    print(f\"    Throughput:  {actual_throughput:.0f} embeddings/sec\")\n",
        "    print(f\"    Avg latency: {embed_time/rows_to_embed*1000:.2f} ms/embedding\")\n",
        "    print(f\"    Content processed: {total_content_chars:,} chars (~{estimated_total_tokens:,.0f} tokens)\")\n",
        "    print(f\"    Token throughput:  {tokens_per_sec:,.0f} tokens/sec ({chars_per_sec:,.0f} chars/sec)\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n\u2713 Already have {embedded_rows:,} embeddings - skipping\")"
      ],
      "id": "3d9878b4"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aa835ac1",
      "metadata": {},
      "source": [
        "## Step 7: Create Vector Index (HNSW)\n",
        "\n",
        "A **vector index** is what makes similarity search fast. Without it, every query must compute `DOT_PRODUCT()` against **all** rows (a full table scan). With an HNSW index, SingleStore uses **Approximate Nearest Neighbor (ANN)** search that runs in sub-linear time.\n",
        "\n",
        "### What is HNSW?\n",
        "**Hierarchical Navigable Small World (HNSW)** is a graph-based ANN algorithm. It builds a multi-layer graph where:\n",
        "- Each vector is a node connected to its nearest neighbors\n",
        "- Higher layers have fewer nodes and longer-range connections (for fast coarse search)\n",
        "- Lower layers have more nodes and shorter-range connections (for fine-grained refinement)\n",
        "\n",
        "Search starts at the top layer and \"navigates\" down, like zooming in on a map.\n",
        "\n",
        "### `HNSW_FLAT` vs other types\n",
        "`HNSW_FLAT` stores the **original full-precision vectors** (no compression). This gives **exact dot-product scores** while still getting the speed benefits of the HNSW graph navigation. Use `HNSW_PQ` (product quantization) if you need to save memory at the cost of some accuracy.\n",
        "\n",
        "### Important: Query pattern for index usage\n",
        "For SingleStore to use the vector index, your query **must** follow this pattern:\n",
        "```sql\n",
        "SELECT ..., DOT_PRODUCT(vector_col, query_vec) AS score\n",
        "FROM table\n",
        "ORDER BY score DESC\n",
        "LIMIT k\n",
        "```\n",
        "Adding `WHERE` filters on non-vector columns (e.g., `score >= 3`) may cause the optimizer to bypass the vector index and fall back to a full scan. See the search endpoint below for how to handle filtered vector search correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n\u2713 Vector index already exists - skipping\n"
        }
      ],
      "source": [
        "if needs_index:\n",
        "    index_start = time.time()\n",
        "    print(\"\\n\ud83d\udd0d Creating vector index...\")\n",
        "\n",
        "    conn = s2.connect()\n",
        "    try:\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "            cur.execute(f\"\"\"\n",
        "                ALTER TABLE {TABLE_NAME}\n",
        "                ADD VECTOR INDEX vindex_review_embedding(review_embedding)\n",
        "                INDEX_OPTIONS '{{\"index_type\":\"HNSW_FLAT\"}}'\n",
        "            \"\"\")\n",
        "        conn.commit()\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    index_time = time.time() - index_start\n",
        "    print(f\"\u2713 Vector index created in {index_time:.2f}s\")\n",
        "else:\n",
        "    print(\"\\n\u2713 Vector index already exists - skipping\")"
      ],
      "id": "ed29ff23"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "24b2ba07",
      "metadata": {},
      "source": [
        "## Step 8: Verify Vector Index Usage\n",
        "\n",
        "Use `EXPLAIN` to confirm that the query optimizer is actually using the HNSW vector index. Look for **`VECTOR_KNN_SCAN`** or **`ANN`** in the query plan \u2014 this proves the index is being used for approximate nearest-neighbor search rather than a full table scan.\n",
        "\n",
        "If you see a regular `TABLE SCAN` instead, the query pattern may not match what the optimizer expects (see the note in Step 7 above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9c1e0cad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\ud83d\udd0d Verifying vector index usage with EXPLAIN...\n\nEXPLAIN output:\n--------------------------------------------------------------------------------\n(\"Project [raw_proj.id, raw_proj.score, LEFT(raw_proj.full_review_text,80) AS preview, DOT_PRODUCT(raw_proj.review_embedding,(x'18678cbd5f5900bd71e8c2bb9e36593b433a123cec48033ddd48e23c3a3dd0bc8f97583d9f088ebcdb7f6d3c15bf2a3dd8b7f83c92c3a9bc369c4dbd538f0e3d9b2c023d015dd2bc1efecebbf1a7cabc6886c93ce28ea9bc8b4b19bd9f19083ce2a084bc43c8f73b1d1c26bd386585bd5c860fbc96391dbdbff12c3de7a1213d79d50abd2ee435bd573c2b3d1debb13d52cc353c7398833c4c934b3c556f193dc1404d3c6b8d093ca71644bbce123c3d83a818bc5ed112bc7896debc3a5d633...] est_rows:5\",)\n(\"Sort [DOT_PRODUCT(raw_proj.review_embedding,(x'18678cbd5f5900bd71e8c2bb9e36593b433a123cec48033ddd48e23c3a3dd0bc8f97583d9f088ebcdb7f6d3c15bf2a3dd8b7f83c92c3a9bc369c4dbd538f0e3d9b2c023d015dd2bc1efecebbf1a7cabc6886c93ce28ea9bc8b4b19bd9f19083ce2a084bc43c8f73b1d1c26bd386585bd5c860fbc96391dbdbff12c3de7a1213d79d50abd2ee435bd573c2b3d1debb13d52cc353c7398833c4c934b3c556f193dc1404d3c6b8d093ca71644bbce123c3d83a818bc5ed112bc7896debc3a5d633d283ed43a4775d7bc8408b33cbe28b8ba301a22bc1f41183c15949b3bb622bc3b7d5e343d620...]\",)\n('HashJoin',)\n('|---HashTableProbe [raw_proj.id = index_projector.OSJ_IND_id_1]',)\n('|   HashTableBuild alias:index_projector',)\n('|   Project [index_projector_0.OSJ_IND_id_1] est_rows:5',)\n('|   TableScan 1tmp AS index_projector_0 storage:list stream:yes est_table_rows:5 est_filtered:5',)\n('|   Project [remote_0.OSJ_IND_id_1] est_rows:5',)\n(\"|   TopSort limit:5 [DOT_PRODUCT(remote_0.review_embedding,(x'18678cbd5f5900bd71e8c2bb9e36593b433a123cec48033ddd48e23c3a3dd0bc8f97583d9f088ebcdb7f6d3c15bf2a3dd8b7f83c92c3a9bc369c4dbd538f0e3d9b2c023d015dd2bc1efecebbf1a7cabc6886c93ce28ea9bc8b4b19bd9f19083ce2a084bc43c8f73b1d1c26bd386585bd5c860fbc96391dbdbff12c3de7a1213d79d50abd2ee435bd573c2b3d1debb13d52cc353c7398833c4c934b3c556f193dc1404d3c6b8d093ca71644bbce123c3d83a818bc5ed112bc7896debc3a5d633d283ed43a4775d7bc8408b33cbe28b8ba301a22bc1f41183c15949b3bb622bc3b7d5e343d620...]\",)\n('|   Gather partitions:all est_rows:5 alias:remote_0 parallelism_level:segment',)\n('|   Project [product_reviews.id AS OSJ_IND_id_1, product_reviews.review_embedding] est_rows:5',)\n(\"|   TopSort limit:5 [DOT_PRODUCT(product_reviews.review_embedding,(x'18678cbd5f5900bd71e8c2bb9e36593b433a123cec48033ddd48e23c3a3dd0bc8f97583d9f088ebcdb7f6d3c15bf2a3dd8b7f83c92c3a9bc369c4dbd538f0e3d9b2c023d015dd2bc1efecebbf1a7cabc6886c93ce28ea9bc8b4b19bd9f19083ce2a084bc43c8f73b1d1c26bd386585bd5c860fbc96391dbdbff12c3de7a1213d79d50abd2ee435bd573c2b3d1debb13d52cc353c7398833c4c934b3c556f193dc1404d3c6b8d093ca71644bbce123c3d83a818bc5ed112bc7896debc3a5d633d283ed43a4775d7bc8408b33cbe28b8ba301a22bc1f41183c15949b3bb622bc3b7d5e...]\",)\n(\"|   ColumnStoreFilter [INTERNAL_VECTOR_SEARCH(0, (x'18678cbd5f5900bd71e8c2bb9e36593b433a123cec48033ddd48e23c3a3dd0bc8f97583d9f088ebcdb7f6d3c15bf2a3dd8b7f83c92c3a9bc369c4dbd538f0e3d9b2c023d015dd2bc1efecebbf1a7cabc6886c93ce28ea9bc8b4b19bd9f19083ce2a084bc43c8f73b1d1c26bd386585bd5c860fbc96391dbdbff12c3de7a1213d79d50abd2ee435bd573c2b3d1debb13d52cc353c7398833c4c934b3c556f193dc1404d3c6b8d093ca71644bbce123c3d83a818bc5ed112bc7896debc3a5d633d283ed43a4775d7bc8408b33cbe28b8ba301a22bc1f41183c15949b3bb622bc3b7d5e343d62041e3d760637b...]\",)\n('|   ColumnStoreScan vector_search_demo.product_reviews, SORT KEY id (id) table_type:sharded_columnstore est_table_rows:10,000 est_filtered:10,000',)\n('Gather partitions:all est_rows:10,000 alias:raw_proj parallelism_level:segment',)\n('Project [remote_1.id, remote_1.score, remote_1.full_review_text, remote_1.review_embedding] est_rows:10,000',)\n('ColumnStoreScan vector_search_demo.product_reviews AS remote_1, SORT KEY id (id) table_type:sharded_columnstore est_table_rows:10,000 est_filtered:10,000',)\n--------------------------------------------------------------------------------\n\n\u2705 Vector index IS being used (ANN search)\n"
        }
      ],
      "source": [
        "# Verify the vector index is being used by the query optimizer\n",
        "print(\"\ud83d\udd0d Verifying vector index usage with EXPLAIN...\\n\")\n",
        "\n",
        "conn = s2.connect()\n",
        "try:\n",
        "    with conn.cursor() as cur:\n",
        "        cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "\n",
        "        # Generate a sample query embedding for the EXPLAIN\n",
        "        cur.execute(f\"\"\"\n",
        "            SELECT JSON_ARRAY_UNPACK_F64(\n",
        "                cluster.EMBED_TEXT('sample query text', model => '{EMBEDDING_MODEL}')\n",
        "            ) :> VECTOR({VECTOR_DIMENSIONS}) as query_embedding\n",
        "        \"\"\")\n",
        "        sample_embedding = cur.fetchone()[0]\n",
        "\n",
        "        # Run EXPLAIN on the vector search query\n",
        "        cur.execute(f\"\"\"\n",
        "            EXPLAIN SELECT\n",
        "                id, score,\n",
        "                LEFT(full_review_text, 80) as preview,\n",
        "                DOT_PRODUCT(review_embedding, %s) as similarity\n",
        "            FROM {TABLE_NAME}\n",
        "            ORDER BY similarity DESC\n",
        "            LIMIT 5\n",
        "        \"\"\", (sample_embedding,))\n",
        "\n",
        "        explain_results = cur.fetchall()\n",
        "\n",
        "        print(\"EXPLAIN output:\")\n",
        "        print(\"-\" * 80)\n",
        "        for row in explain_results:\n",
        "            print(row)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Check if vector index is being used\n",
        "        explain_text = str(explain_results).lower()\n",
        "        if 'vector' in explain_text or 'ann' in explain_text or 'hnsw' in explain_text:\n",
        "            print(\"\\n\u2705 Vector index IS being used (ANN search)\")\n",
        "        else:\n",
        "            print(\"\\n\u26a0\ufe0f  Vector index may NOT be used \u2014 check the EXPLAIN output above\")\n",
        "            print(\"    Ensure the query follows: ORDER BY DOT_PRODUCT(...) DESC LIMIT k\")\n",
        "finally:\n",
        "    conn.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9846d0ad",
      "metadata": {},
      "source": [
        "## Step 9: Test Vector Search\n",
        "\n",
        "Run a simple semantic search to verify everything works end-to-end. The flow is:\n",
        "\n",
        "1. **Embed the query** : call `EMBED_TEXT()` on the search text to get a query vector\n",
        "2. **Compute similarity** : calculate `DOT_PRODUCT(review_embedding, query_vec)` for each row\n",
        "3. **Rank and return** : `ORDER BY similarity DESC LIMIT k` returns the top matches\n",
        "\n",
        "Because we created an HNSW index, Step 2-3 use **approximate nearest-neighbor** search instead of scanning every row. For 10,000 rows the difference is small, but for millions of rows this is the difference between milliseconds and seconds.\n",
        "\n",
        "> **Note**: Set `TEST_VECTOR_SEARCH = True` in the configuration cell to enable this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\u2705 Skipped Vector search check\n"
        }
      ],
      "source": [
        "if TEST_VECTOR_SEARCH:\n",
        "    test_start = time.time()\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TESTING VECTOR SEARCH\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    test_query = \"delicious and healthy food\"\n",
        "\n",
        "    conn = s2.connect()\n",
        "    try:\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "\n",
        "            # Embed query using the same model used for stored embeddings\n",
        "            cur.execute(f\"\"\"\n",
        "                SELECT JSON_ARRAY_UNPACK_F64(\n",
        "                    cluster.EMBED_TEXT(%s, model => '{EMBEDDING_MODEL}')\n",
        "                ) :> VECTOR({VECTOR_DIMENSIONS}) as query_embedding\n",
        "            \"\"\", (test_query,))\n",
        "            query_embedding = cur.fetchone()[0]\n",
        "\n",
        "            # Vector similarity search \u2014 this pattern ensures HNSW index usage:\n",
        "            # ORDER BY DOT_PRODUCT(...) DESC LIMIT k  (no WHERE filters on non-vector columns)\n",
        "            search_start = time.time()\n",
        "            cur.execute(f\"\"\"\n",
        "                SELECT\n",
        "                    id, score,\n",
        "                    LEFT(full_review_text, 80) as preview,\n",
        "                    DOT_PRODUCT(review_embedding, %s) as similarity\n",
        "                FROM {TABLE_NAME}\n",
        "                ORDER BY similarity DESC\n",
        "                LIMIT 5\n",
        "            \"\"\", (query_embedding,))\n",
        "\n",
        "            results = cur.fetchall()\n",
        "            search_time = time.time() - search_start\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "    print(f\"\\nQuery: '{test_query}'\")\n",
        "    print(f\"Search time: {search_time*1000:.2f} ms\\n\")\n",
        "\n",
        "    for i, row in enumerate(results, 1):\n",
        "        print(f\"{i}. [Score: {row[3]:.4f}] [{row[1]}\u2605] {row[2]}...\")\n",
        "\n",
        "    test_time = time.time() - test_start\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"\u2705 Vector search working! (total test time: {test_time:.2f}s)\")\n",
        "else:\n",
        "    print(\"\u2705 Skipped Vector search check\")"
      ],
      "id": "a53807ac"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "12d9f2cd",
      "metadata": {},
      "source": [
        "## Step 10: Pipeline Summary\n",
        "\n",
        "Review the full execution timing breakdown. This helps you understand where time is spent and optimize for your use case:\n",
        "- **Cold start** (install + import) is one-time overhead\n",
        "- **Embedding generation** is typically the bottleneck, scale workers and batch size to match your cluster capacity\n",
        "- **Index creation** is fast and only happens once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n================================================================================\n\ud83d\udcca NOTEBOOK EXECUTION SUMMARY\n================================================================================\n\n\ud83d\udccb Configuration:\n  Dataset size:        10,000 rows\n  Embedding model:     titan-embed-text-v2-0\n  Vector dimensions:   1024\n  Max embedding tokens:512 (~2048 chars)\n  Database:            vector_search_demo\n  Table:               product_reviews\n\n\u23f1\ufe0f  Execution Timing:\n  Setup:\n    - Dependency install: 4.3s\n    - Library imports:    0.07s\n  State check:            816.31 ms\n  Database/Table setup:   skipped (already exists)\n  Data ingestion:         skipped (10,000 rows exist)\n  Embedding generation:   98.78s (48 emb/s)\n    - Content size:       2,059,370.0 chars (~514,842 tokens)\n    - Avg text/row:       434 chars (~108 tokens)\n    - Text range:         76.0 - 2,050.0 chars/row\n    - Token throughput:   5,212 tokens/sec\n  Index creation:         skipped (index exists)\n  Vector search test:     skipped\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ud83c\udfaf Total Pipeline Time:      106.49s (1.77 minutes)\n   Cold Start Overhead:      4.33s (4.1%)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ud83d\udcc8 Performance Metrics (if applicable):\n  Embedding rate:         48 embeddings/sec\n  Embedding throughput:   48 embeddings/sec\n  Token throughput:       5,212 tokens/sec\n  Chars throughput:       20,848 chars/sec\n"
        }
      ],
      "source": [
        "# NOTEBOOK EXECUTION SUMMARY\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\ud83d\udcca NOTEBOOK EXECUTION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Configuration summary\n",
        "print(f\"\\n\ud83d\udccb Configuration:\")\n",
        "print(f\"  Dataset size:        {SAMPLE_SIZE:,} rows\")\n",
        "print(f\"  Embedding model:     {EMBEDDING_MODEL}\")\n",
        "print(f\"  Vector dimensions:   {VECTOR_DIMENSIONS}\")\n",
        "print(f\"  Max embedding tokens:{MAX_EMBEDDING_TOKENS} (~{MAX_EMBEDDING_TOKENS * APPROX_CHARS_PER_TOKEN} chars)\")\n",
        "print(f\"  Database:            {DATABASE_NAME}\")\n",
        "print(f\"  Table:               {TABLE_NAME}\")\n",
        "\n",
        "# Timing breakdown\n",
        "print(f\"\\n\u23f1\ufe0f  Execution Timing:\")\n",
        "print(f\"  Setup:\")\n",
        "print(f\"    - Dependency install: {install_time:.1f}s\")\n",
        "print(f\"    - Library imports:    {import_time:.2f}s\")\n",
        "\n",
        "# State check time (if available)\n",
        "if 'state_check_time' in globals():\n",
        "    print(f\"  State check:            {state_check_time*1000:.2f} ms\")\n",
        "\n",
        "# Database/Table setup\n",
        "if 'setup_time' in globals():\n",
        "    print(f\"  Database/Table setup:   {setup_time:.2f}s\")\n",
        "else:\n",
        "    print(f\"  Database/Table setup:   skipped (already exists)\")\n",
        "\n",
        "# Data ingestion\n",
        "if 'data_time' in globals():\n",
        "    print(f\"  Data ingestion:         {data_time:.2f}s\")\n",
        "    if 'download_time' in globals():\n",
        "        print(f\"    - Download:           {download_time:.2f}s\")\n",
        "    if 'prep_time' in globals():\n",
        "        print(f\"    - Prepare:            {prep_time:.2f}s\")\n",
        "    if 'insert_time' in globals():\n",
        "        rows_rate = rows_needed/insert_time if 'rows_needed' in globals() and insert_time > 0 else 0\n",
        "        print(f\"    - Insert:             {insert_time:.2f}s ({rows_rate:.0f} rows/s)\")\n",
        "    if 'num_truncated' in globals():\n",
        "        print(f\"    - Reviews truncated:  {num_truncated:,}/{SAMPLE_SIZE:,} ({num_truncated/SAMPLE_SIZE*100:.1f}%)\")\n",
        "else:\n",
        "    print(f\"  Data ingestion:         skipped ({total_rows:,} rows exist)\")\n",
        "\n",
        "# Embedding generation\n",
        "if 'embed_time' in globals():\n",
        "    emb_rate = rows_to_embed/embed_time if 'rows_to_embed' in globals() and embed_time > 0 else 0\n",
        "    print(f\"  Embedding generation:   {embed_time:.2f}s ({emb_rate:.0f} emb/s)\")\n",
        "    if 'total_content_chars' in globals() and total_content_chars > 0:\n",
        "        print(f\"    - Content size:       {total_content_chars:,} chars (~{estimated_total_tokens:,.0f} tokens)\")\n",
        "        print(f\"    - Avg text/row:       {avg_content_chars:,.0f} chars (~{avg_content_chars/APPROX_CHARS_PER_TOKEN:,.0f} tokens)\")\n",
        "        print(f\"    - Text range:         {min_content_chars:,} - {max_content_chars:,} chars/row\")\n",
        "        print(f\"    - Token throughput:   {estimated_total_tokens/embed_time:,.0f} tokens/sec\")\n",
        "else:\n",
        "    print(f\"  Embedding generation:   skipped ({embedded_rows:,} embeddings exist)\")\n",
        "\n",
        "# Index creation\n",
        "if 'index_time' in globals():\n",
        "    print(f\"  Index creation:         {index_time:.2f}s\")\n",
        "else:\n",
        "    print(f\"  Index creation:         skipped (index exists)\")\n",
        "\n",
        "# Vector search test\n",
        "if 'test_time' in globals():\n",
        "    print(f\"  Vector search test:     {test_time:.2f}s\")\n",
        "else:\n",
        "    print(f\"  Vector search test:     skipped\")\n",
        "\n",
        "# Total time summary\n",
        "print(f\"\\n{'\u2500' * 80}\")\n",
        "print(f\"\ud83c\udfaf Total Pipeline Time:      {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "cold_start_overhead = install_time + import_time\n",
        "print(f\"   Cold Start Overhead:      {cold_start_overhead:.2f}s ({cold_start_overhead/total_time*100:.1f}%)\")\n",
        "print(f\"{'\u2500' * 80}\")\n",
        "\n",
        "# Performance metrics\n",
        "print(f\"\\n\ud83d\udcc8 Performance Metrics (if applicable):\")\n",
        "\n",
        "if 'data_time' in globals() and 'embed_time' in globals():\n",
        "    core_time = data_time + embed_time\n",
        "    print(f\"  End-to-end throughput:  {SAMPLE_SIZE/core_time:.0f} rows/sec (excluding setup)\")\n",
        "elif 'data_time' in globals():\n",
        "    print(f\"  Data processing rate:   {rows_needed/data_time:.0f} rows/sec\")\n",
        "elif 'embed_time' in globals():\n",
        "    print(f\"  Embedding rate:         {rows_to_embed/embed_time:.0f} embeddings/sec\")\n",
        "\n",
        "if 'insert_time' in globals() and 'rows_needed' in globals():\n",
        "    print(f\"  Data ingest rate:       {rows_needed/insert_time:.0f} rows/sec\")\n",
        "\n",
        "if 'embed_time' in globals() and 'rows_to_embed' in globals():\n",
        "    print(f\"  Embedding throughput:   {rows_to_embed/embed_time:.0f} embeddings/sec\")\n",
        "    if 'total_content_chars' in globals() and total_content_chars > 0:\n",
        "        print(f\"  Token throughput:       {estimated_total_tokens/embed_time:,.0f} tokens/sec\")\n",
        "        print(f\"  Chars throughput:       {total_content_chars/embed_time:,.0f} chars/sec\")"
      ],
      "id": "341b4d2d"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a8440f17",
      "metadata": {},
      "source": [
        "## Step 11: Define Cloud Function API\n",
        "\n",
        "Now we wrap the vector search in a **FastAPI** application that can be deployed as a [SingleStore Cloud Function](https://docs.singlestore.com/cloud/container-services/cloud-functions/). This gives you a production-ready REST API endpoint.\n",
        "\n",
        "### Endpoints\n",
        "| Method | Path | Description |\n",
        "|--------|------|-------------|\n",
        "| `GET` | `/` | API info and available endpoints |\n",
        "| `GET` | `/health` | Health check with database statistics |\n",
        "| `POST` | `/search` | Semantic vector search |\n",
        "\n",
        "This API demonstrates `EMBED_TEXT()` + `DOT_PRODUCT()` vector search as a building block. You can extend it with application-specific filters, pagination, or business logic for your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n================================================================================\n\ud83d\ude80 DEFINING CLOUD FUNCTION API\n================================================================================\n\n\u2713 FastAPI application defined\n\nEndpoints configured:\n  GET  /          - API information\n  GET  /health    - Health check with database stats\n  POST /search    - Semantic vector search\n"
        }
      ],
      "source": [
        "# CLOUD FUNCTION API DEFINITION\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\ud83d\ude80 DEFINING CLOUD FUNCTION API\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, List\n",
        "import singlestoredb as s2\n",
        "import singlestoredb.apps as apps\n",
        "\n",
        "# Request/Response Models\n",
        "class SearchRequest(BaseModel):\n",
        "    query: str = Field(..., description=\"Search query text\", min_length=1, max_length=500)\n",
        "    limit: Optional[int] = Field(10, description=\"Number of results to return\", ge=1, le=100)\n",
        "\n",
        "class SearchResult(BaseModel):\n",
        "    id: int\n",
        "    product_id: str\n",
        "    review_text: str\n",
        "    star_rating: int\n",
        "    similarity_score: float\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    status: str\n",
        "    query: str\n",
        "    results_count: int\n",
        "    results: List[SearchResult]\n",
        "    latency_ms: float\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str\n",
        "    database: str\n",
        "    total_reviews: int\n",
        "    embedded_reviews: int\n",
        "    index_status: str\n",
        "\n",
        "# Create FastAPI Application\n",
        "app = FastAPI(\n",
        "    title=\"Product Review Vector Search API\",\n",
        "    version=\"1.0.0\",\n",
        "    description=\"Semantic search for product reviews using SingleStore vector embeddings\"\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"API information endpoint\"\"\"\n",
        "    return {\n",
        "        \"service\": \"Product Review Vector Search API\",\n",
        "        \"version\": \"1.0.0\",\n",
        "        \"model\": EMBEDDING_MODEL,\n",
        "        \"dimensions\": VECTOR_DIMENSIONS,\n",
        "        \"database\": DATABASE_NAME,\n",
        "        \"endpoints\": {\n",
        "            \"GET /\": \"API information\",\n",
        "            \"GET /health\": \"Health check and database stats\",\n",
        "            \"POST /search\": \"Semantic vector search\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "async def health():\n",
        "    \"\"\"Health check endpoint with database statistics\"\"\"\n",
        "    try:\n",
        "        with s2.connect() as conn:\n",
        "            with conn.cursor() as cur:\n",
        "                cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "\n",
        "                cur.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME}\")\n",
        "                total = cur.fetchone()[0]\n",
        "\n",
        "                cur.execute(f\"SELECT COUNT(*) FROM {TABLE_NAME} WHERE review_embedding IS NOT NULL\")\n",
        "                embedded = cur.fetchone()[0]\n",
        "\n",
        "                cur.execute(f\"\"\"\n",
        "                    SELECT COUNT(*) FROM information_schema.statistics\n",
        "                    WHERE table_schema = '{DATABASE_NAME}'\n",
        "                    AND table_name = '{TABLE_NAME}'\n",
        "                    AND index_name = 'vindex_review_embedding'\n",
        "                \"\"\")\n",
        "                index_exists = cur.fetchone()[0] > 0\n",
        "\n",
        "        return HealthResponse(\n",
        "            status=\"healthy\",\n",
        "            database=DATABASE_NAME,\n",
        "            total_reviews=total,\n",
        "            embedded_reviews=embedded,\n",
        "            index_status=\"active\" if index_exists else \"missing\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=503, detail=f\"Service unavailable: {str(e)}\")\n",
        "\n",
        "@app.post(\"/search\", response_model=SearchResponse)\n",
        "async def search(request: SearchRequest):\n",
        "    \"\"\"\n",
        "    Semantic search endpoint for product reviews.\n",
        "\n",
        "    Embeds the query text using EMBED_TEXT() with the same model used for\n",
        "    stored embeddings, then performs DOT_PRODUCT vector similarity search\n",
        "    using the HNSW index to return the top-k most relevant reviews.\n",
        "    \"\"\"\n",
        "    search_start = time.time()\n",
        "\n",
        "    try:\n",
        "        with s2.connect() as conn:\n",
        "            with conn.cursor() as cur:\n",
        "                cur.execute(f\"USE {DATABASE_NAME}\")\n",
        "\n",
        "                # Embed the query using the same model as stored embeddings\n",
        "                cur.execute(f\"\"\"\n",
        "                    SELECT JSON_ARRAY_UNPACK_F64(\n",
        "                        cluster.EMBED_TEXT(%s, model => '{EMBEDDING_MODEL}')\n",
        "                    ) :> VECTOR({VECTOR_DIMENSIONS})\n",
        "                \"\"\", (request.query,))\n",
        "                query_embedding = cur.fetchone()[0]\n",
        "\n",
        "                # Vector similarity search using HNSW index\n",
        "                cur.execute(f\"\"\"\n",
        "                    SELECT\n",
        "                        id,\n",
        "                        product_id,\n",
        "                        full_review_text,\n",
        "                        score,\n",
        "                        DOT_PRODUCT(review_embedding, %s) as similarity\n",
        "                    FROM {TABLE_NAME}\n",
        "                    ORDER BY similarity DESC\n",
        "                    LIMIT %s\n",
        "                \"\"\", (query_embedding, request.limit))\n",
        "\n",
        "                rows = cur.fetchall()\n",
        "\n",
        "                results = [\n",
        "                    SearchResult(\n",
        "                        id=row[0],\n",
        "                        product_id=row[1],\n",
        "                        review_text=row[2],\n",
        "                        star_rating=row[3],\n",
        "                        similarity_score=float(row[4])\n",
        "                    ) for row in rows\n",
        "                ]\n",
        "\n",
        "        latency = (time.time() - search_start) * 1000\n",
        "\n",
        "        return SearchResponse(\n",
        "            status=\"success\",\n",
        "            query=request.query,\n",
        "            results_count=len(results),\n",
        "            results=results,\n",
        "            latency_ms=round(latency, 2)\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Search failed: {str(e)}\")\n",
        "\n",
        "print(\"\\n\u2713 FastAPI application defined\")\n",
        "print(\"\\nEndpoints configured:\")\n",
        "print(\"  GET  /          - API information\")\n",
        "print(\"  GET  /health    - Health check with database stats\")\n",
        "print(\"  POST /search    - Semantic vector search\")"
      ],
      "id": "d5bb9449"
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f208ae73",
      "metadata": {},
      "source": [
        "## Step 12: Deploy as Cloud Function\n",
        "\n",
        "`singlestoredb.apps.run_function_app()` deploys the FastAPI app as a **SingleStore Cloud Function**. This:\n",
        "- Packages the FastAPI app into a containerized microservice\n",
        "- Hosts it on SingleStore's infrastructure with automatic HTTPS\n",
        "- Provides a URL you can call from any HTTP client\n",
        "\n",
        "### How to Call the Deployed Endpoint\n",
        "\n",
        "After deployment, you'll need two things from the **SingleStore Portal UI**:\n",
        "\n",
        "1. **Endpoint URL** : Find it in the Cloud Functions section of the portal after deployment\n",
        "2. **API Key** : Generate one in the portal under **Cloud Functions > API Keys**. This key is required for all requests.\n",
        "\n",
        "All requests must include a `Bearer` authorization header with your API key:\n",
        "\n",
        "```bash\n",
        "# Health check\n",
        "curl https://<your-endpoint-url>/health \\\n",
        "  -H \"Authorization: Bearer <your-api-key>\"\n",
        "\n",
        "# Semantic search\n",
        "curl -X POST https://<your-endpoint-url>/search \\\n",
        "  -H \"Authorization: Bearer <your-api-key>\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"query\": \"delicious healthy snack\", \"limit\": 5}'\n",
        "```\n",
        "\n",
        "> **Note**: Replace `<your-endpoint-url>` and `<your-api-key>` with the actual values from the SingleStore Portal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\n================================================================================\n\ud83d\ude80 DEPLOYING CLOUD FUNCTION\n================================================================================\nCloud function available at https://apps.us-east-1.cloud.singlestore.com/notebooks/InteractiveNotebook/c0f26f27-5554-4e4f-b293-bbe4554f13ad/app/docs?authToken=eyJhbGciOiJFUzUxMiIsImtpZCI6IjhhNmVjNWFmLThlNWEtNDQxOS04NmM4LWRkMDkxN2U1YWNlMSIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI1YjQ1OTgxYy04YjA5LTRlYWQtYmVjMC0wOTU0N2Q3YjlhOTciLCJhdWQiOlsibm92YXB1YmxpYyJdLCJleHAiOjE3NzIwNzgyNjYsIm5iZiI6MTc3MjA0NzY2NiwiaWF0IjoxNzcyMDQ3NjY2LCJqdGkiOiI3YmMwNmY2Zi1mY2YxLTQ1NTMtODljMC1kOTc1NTgzNzE0NDciLCJjb250YWluZXJJRCI6ImMwZjI2ZjI3LTU1NTQtNGU0Zi1iMjkzLWJiZTQ1NTRmMTNhZCJ9.AGxGgw3hGCnWWqaVnbwLP8eaJRNWsOMp6uchpIrQ7TuBoVXr-5r2kO96g0RTNKe-D9RlRSZWq7dVUFjUJZVJp9esAEpk9BqWHZWrs7XCdx8TkYQXgmiQMMBONGC8ffI_-jcD_odWrpiT3xOcny1QYr0lnOoUwOOvHwr4Y6ru_q8YU1XG\n"
        }
      ],
      "source": [
        "# ========================================\n",
        "# DEPLOY CLOUD FUNCTION\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\ud83d\ude80 DEPLOYING CLOUD FUNCTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Deploy the function app\n",
        "connection_info = await apps.run_function_app(app)"
      ],
      "id": "f94a9798"
    },
    {
      "id": "94d7a5a0",
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
